{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uA3nmwPwyBMX",
    "colab_type": "text"
   },
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stVyicl_yGIl",
    "colab_type": "text"
   },
   "source": [
    "**Your name: Tyler Lott**\n",
    "\n",
    "**A-Number: A02230980**\n",
    "\n",
    "In this homework, we will build a model based on real house sale data from a [Kaggle competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). This notebook contains codes to download the dataset, build and train a baseline model, and save the results in the submission format. Your jobs \n",
    "\n",
    "1.   Implement the preprocessing code\n",
    "\n",
    "2.   Developing a better model to reduce the prediction error. You can try any models you know.\n",
    "\n",
    "3.   Submitting your results into Kaggle and take a sceenshot of your score. Then replace the following image URL with your screenshot.\n",
    "\n",
    "![](https://drive.google.com/uc?export=download&id=1vZ85KWX3oaE2vOAk1FUNYAJF2kFLGgao)\n",
    "\n",
    "4.   Submit the .IPYNB file to Canvas.\n",
    "    - Missing the output after execution may hurt your grade.\n",
    "\n",
    "## Accessing and Reading Data Sets\n",
    "\n",
    "The competition data is separated into training and test sets. Each record includes the property values of the house and attributes such as street type, year of construction, roof type, basement condition. The data includes multiple datatypes, including integers (year of construction), discrete labels (roof type), floating point numbers, etc.; Some data is missing and is thus labeled 'na'. The price of each house, namely the label, is only included in the training data set (it's a competition after all). The 'Data' tab on the competition tab has links to download the data.\n",
    "\n",
    "We will read and process the data using `pandas`, an [efficient data analysis toolkit](http://pandas.pydata.org/pandas-docs/stable/). Make sure you have `pandas` installed for the experiments in this section."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wx9ga3uzx9Mo",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "outputId": "b112c1f6-93fe-43c5-85ff-a5d202ad91f8",
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# If pandas is not installed, please uncomment and run the following line:\n",
    "# !pip install pandas"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MYPxXoI20b98",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgH-Yitf22sc",
    "colab_type": "text"
   },
   "source": [
    "We downloaded the data into the current directory. To load the two CSV (Comma Separated Values) files containing training and test data respectively we use Pandas."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SnC1qnsy1TiZ",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "outputId": "c10f9ba0-61cf-41d5-c8de-cd2145a408a6",
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "!wget https://raw.githubusercontent.com/d2l-ai/data/master/kaggle_house_pred_test.csv\n",
    "!wget https://raw.githubusercontent.com/d2l-ai/data/master/kaggle_house_pred_train.csv"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "'wget' is not recognized as an internal or external command,\noperable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\noperable program or batch file.\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8F0Yfc2X2CPi",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "train_data = pd.read_csv('kaggle_house_pred_train.csv')\n",
    "test_data = pd.read_csv('kaggle_house_pred_test.csv')"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nV7ywmUB25_h",
    "colab_type": "text"
   },
   "source": [
    "The training data set includes 1,460 examples, 80 features, and 1 label., the test data contains 1,459 examples and 80 features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "THmf3WyP27PM",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "outputId": "2fd7c73d-e03d-403f-bbea-328410ce9630",
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(1460, 81)\n(1459, 80)\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fxMHe1k3BzZ",
    "colab_type": "text"
   },
   "source": [
    "Let’s take a look at the first 4 and last 2 features as well as the label (SalePrice) from the first 4 examples:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6sfz9Gtl3C1b",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "outputId": "336e9d09-15d6-4a07-f1fb-26308424d35c",
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]]"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "   Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice\n0   1          60       RL         65.0       WD        Normal     208500\n1   2          20       RL         80.0       WD        Normal     181500\n2   3          60       RL         68.0       WD        Normal     223500\n3   4          70       RL         60.0       WD       Abnorml     140000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwTwT20T3KAt",
    "colab_type": "text"
   },
   "source": [
    "We can see that in each example, the first feature is the ID. This helps the model identify each training example. While this is convenient, it doesn't carry any information for prediction purposes. Hence we remove it from the dataset before feeding the data into the network."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0WZmJ9Cp3JeZ",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ui1WxgCg3Q1E",
    "colab_type": "text"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "As stated above, we have a wide variety of datatypes. Before we feed it into a deep network we need to perform some amount of processing. Let's start with the numerical features. We begin by replacing missing values with the mean. This is a reasonable strategy if features are missing at random. To adjust them to a common scale we rescale them to zero mean and unit variance. This is accomplished as follows:\n",
    "\n",
    "$$x \\leftarrow \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "To check that this transforms $x$ to data with zero mean and unit variance simply calculate $\\mathbf{E}[(x-\\mu)/\\sigma] = (\\mu - \\mu)/\\sigma = 0$. To check the variance we use $\\mathbf{E}[(x-\\mu)^2] = \\sigma^2$ and thus the transformed variable has unit variance. The reason for 'normalizing' the data is that it brings all features to the same order of magnitude. After all, we do not know *a priori* which features are likely to be relevant. Hence it makes sense to treat them equally."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R4ZFD_zD3STa",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "print(len(train_data.columns))"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "81\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "## your code here\n",
    "\n",
    "# Combine DataFrames\n",
    "all_features = pd.concat([train_data, test_data], axis=0, ignore_index=True)\n",
    "\n",
    "# We aren't going to modify the sale price or ID because it is what we are predicting\n",
    "salePrice = all_features.pop(\"SalePrice\")\n",
    "ID = train_data.pop(\"Id\")\n",
    "\n",
    "\n",
    "# separating all of the numerical and categorical data types\n",
    "numerical_data = all_features.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "\n",
    "categorical_data = all_features.select_dtypes(include=[\"object\"])\n",
    "\n",
    "# Normalizing the numerical data\n",
    "mean = numerical_data.mean()\n",
    "std = numerical_data.std()\n",
    "\n",
    "# mapping to gaussian distribution\n",
    "numerical_data = (numerical_data - mean) / std\n",
    "\n",
    "# check to see if the mean is zero\n",
    "# print((numerical_data.mean() - numerical_data.mean()) / std)\n",
    "\n",
    "# check to see if the variance is one\n",
    "# print(numerical_data.std())\n",
    "\n",
    "\n",
    "# combining the data types back together\n",
    "all_features = pd.concat([numerical_data, categorical_data], axis=1, sort=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# after standardizing the data all means vanish, hence we can set missing values to 0\n",
    "all_features = all_features.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiM9DPGK3xGm",
    "colab_type": "text"
   },
   "source": [
    "Next we deal with discrete values. This includes variables such as 'MSZoning'. We replace them by a one-hot encoding in the same manner as how we transformed multiclass classification data into a vector of $0$ and $1$. For instance, 'MSZoning' assumes the values 'RL' and 'RM'. They map into vectors $(1,0)$ and $(0,1)$ respectively. Pandas does this automatically for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## your code here\n",
    "\n",
    "all_features = pd.get_dummies(all_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5adavFft3yc9",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "9fcb2f17-81d6-4049-f63b-15579afc98ea",
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "all_features.shape"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(2919, 312)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 12
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftmtYZnK4aTz",
    "colab_type": "text"
   },
   "source": [
    "You can see that this conversion increases the number of features from 79 to 354. Finally, via the values attribute we can extract the NumPy format from the Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JepAWRlO4J5n",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "n_train = train_data.shape[0]\n",
    "train_features = all_features[:n_train].values\n",
    "test_features = all_features[n_train:].values\n",
    "train_labels = train_data.SalePrice.values.reshape((-1, 1))"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ytjd37es4vtq",
    "colab_type": "text"
   },
   "source": [
    "## Training\n",
    "\n",
    "To get started we train a linear model with squared loss. This will obviously not lead to a competition winning submission but it provides a sanity check to see whether there's meaningful information in the data. It also amounts to a minimum baseline of how well we should expect any 'fancy' model to work."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WED8M73t4wsu",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fBqU5YLF8gxg",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "6d69953c-ae0f-4526-c4fc-90015cdb79a0",
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(train_features, train_labels)\n",
    "reg.score(train_features, train_labels)"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9332973572261254"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 15
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn import svm, tree"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 17
    }
   ],
   "source": [
    "# my regressions\n",
    "\n",
    "# reg1 = Lasso()\n",
    "# reg1.fit(train_features, train_labels)\n",
    "# reg1.score(train_features, train_labels)\n",
    "\n",
    "# reg2 = Ridge()\n",
    "# reg2.fit(train_features, train_labels)\n",
    "# reg2.score(train_features, train_labels)\n",
    "# \n",
    "# reg3 = svm.SVR(kernel=\"rbf\")\n",
    "# reg3.fit(train_features, train_labels)\n",
    "# reg3.score(train_features, train_labels)\n",
    "\n",
    "reg4 = tree.DecisionTreeRegressor()\n",
    "reg4.fit(train_features, train_labels)\n",
    "reg4.score(train_features, train_labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoV4Ox2K9nm3",
    "colab_type": "text"
   },
   "source": [
    "##  Predict and Submit\n",
    "\n",
    "Now that we know what a good choice of hyperparameters should be, we might as well use all the data to train on it (rather than just $1-1/k$ of the data that is used in the crossvalidation slices). The model that we obtain in this way can then be applied to the test set. Saving the estimates in a CSV file will simplify uploading the results to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2Saps5iS9okQ",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "def train_and_pred(test_feature, test_data,):\n",
    "    preds = reg4.predict(test_features)\n",
    "    # reformat it for export to Kaggle\n",
    "    test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])\n",
    "    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)\n",
    "    submission.to_csv('submission.csv', index=False)"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gb2BJDmU-GWU",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "train_and_pred(test_features, test_data)"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8bHvMd1--Tv",
    "colab_type": "text"
   },
   "source": [
    "A file, `submission.csv` will be generated by the code above (CSV is one of the file formats accepted by Kaggle).  Next, we can submit our predictions on Kaggle and compare them to the actual house price (label) on the testing data set, checking for errors. The steps are quite simple:\n",
    "\n",
    "* Log in to the Kaggle website and visit the House Price Prediction Competition page.\n",
    "* Click the “Submit Predictions” or “Late Submission” button on the right.\n",
    "* Click the “Upload Submission File” button in the dashed box at the bottom of the page and select the prediction file you wish to upload.\n",
    "* Click the “Make Submission” button at the bottom of the page to view your results.\n",
    "\n",
    "* First Submission using linear regression\n",
    "![](https://drive.google.com/uc?export=download&id=1kIE-qjvRTq-FAyvVOlacD7Jd1MAf3yBI)\n",
    "\n",
    "* Second Submission using decision trees\n",
    "![](https://drive.google.com/uc?export=download&id=1vZ85KWX3oaE2vOAk1FUNYAJF2kFLGgao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXZ9Wsd__Afe",
    "colab_type": "text"
   },
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW1_house_price_prediction_sol.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}